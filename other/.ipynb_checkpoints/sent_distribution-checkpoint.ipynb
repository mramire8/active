{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sentence Distribution\n",
      "\n",
      "Sentence distribution for IMDB dataset \n",
      "\n",
      "* How many sentences per document in general\n",
      "* How many sentences per document per class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'mramire8'\n",
      "__copyright__ = \"Copyright 2013, ML Lab\"\n",
      "__version__ = \"0.1\"\n",
      "__status__ = \"Development\"\n",
      "\n",
      "import sys\n",
      "import os\n",
      "\n",
      "sys.path.append(os.path.abspath(\".\"))\n",
      "sys.path.append(os.path.abspath(\"../\"))\n",
      "sys.path.append(os.path.abspath(\"../experiment/\"))\n",
      "\n",
      "\n",
      "from experiment.experiment_utils import parse_parameters_mat, clean_html, set_cost_model\n",
      "import argparse\n",
      "import numpy as np\n",
      "from sklearn.datasets.base import Bunch\n",
      "from datautil.load_data import load_from_file\n",
      "from sklearn import linear_model\n",
      "import time\n",
      "\n",
      "from collections import defaultdict, Counter\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "import random\n",
      "import nltk\n",
      "from scipy.sparse import vstack\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "import brewer2mpl\n",
      "import itertools\n",
      "mpl.style.use('bmh')\n",
      "\n",
      "def print_features(coef, names):\n",
      "    \"\"\" Print sorted list of non-zero features/weights. \"\"\"\n",
      "    print \"\\n\".join('%s/%.2f' % (names[j], coef[j]) for j in np.argsort(coef)[::-1] if coef[j] != 0)\n",
      "\n",
      "def sentences_average(pool, vct):\n",
      "   ## COMPUTE: AVERAGE SENTENCES IN DOCUMENTS\n",
      "    tk = vct.build_tokenizer()\n",
      "    allwords = 0.\n",
      "    sum_sent = 0.\n",
      "    average_words = 0\n",
      "    min_sent = 10000\n",
      "    max_sent = 0\n",
      "    for docid, label in zip(pool.remaining, pool.target):\n",
      "\n",
      "        doc = pool.text[docid].replace(\"<br>\", \". \")\n",
      "        doc = doc.replace(\"<br />\", \". \")\n",
      "        isent = sent_detector.tokenize(doc)\n",
      "        sum_sent += len(isent)\n",
      "        min_sent = min(min_sent, len(isent))\n",
      "        max_sent = max(max_sent, len(isent))\n",
      "        for s in sent_detector.tokenize(doc):\n",
      "            average_words += len(tk(s))\n",
      "            allwords += 1\n",
      "\n",
      "    print(\"Average sentences fragments %s\" % (sum_sent / len(pool.target)))\n",
      "    print(\"Min sentences fragments %s\" % min_sent)\n",
      "    print(\"Max sentences fragments %s\" % max_sent)\n",
      "    print(\"Total sentences fragments %s\" % sum_sent)\n",
      "    print(\"Average size of sentence %s\" % (average_words / allwords))\n",
      "\n",
      "\n",
      "def get_data(train, cats, vct, raw):\n",
      "\n",
      "    min_size = None\n",
      "\n",
      "    args.fixk = None\n",
      "    fixk=None\n",
      "\n",
      "    data, vct2 = load_from_file(train, cats, fixk, min_size, vct, raw=raw)\n",
      "\n",
      "    print(\"Data %s\" % args.train)\n",
      "    print(\"Data size %s\" % len(data.train.data))\n",
      "\n",
      "    parameters = parse_parameters_mat(args.cost_model)\n",
      "\n",
      "    print \"Cost Parameters %s\" % parameters\n",
      "\n",
      "    cost_model = set_cost_model(args.cost_function, parameters=parameters)\n",
      "    print \"\\nCost Model: %s\" % cost_model.__class__.__name__\n",
      "\n",
      "    ### SENTENCE TRANSFORMATION\n",
      "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "    ## delete <br> to \".\" to recognize as end of sentence\n",
      "    data.train.data = clean_html(data.train.data)\n",
      "    data.test.data = clean_html(data.test.data)\n",
      "\n",
      "    print(\"Train:{}, Test:{}, {}\".format(len(data.train.data), len(data.test.data), data.test.target.shape[0]))\n",
      "\n",
      "    ## convert document to matrix\n",
      "    data.train.bow = vct.fit_transform(data.train.data)\n",
      "    data.test.bow = vct.transform(data.test.data)\n",
      "    return data\n",
      "\n",
      "\n",
      "####################### MAIN ####################\n",
      "from scipy.sparse import diags\n",
      "\n",
      "\n",
      "def sentence2values(doc_text, sent_detector, score_model, vcn):\n",
      "        np.set_printoptions(precision=4)\n",
      "        sents = sent_detector.tokenize(doc_text)\n",
      "        sents_feat = vcn.transform(sents)\n",
      "        coef = score_model.coef_[0]\n",
      "        dc = diags(coef, 0)\n",
      "\n",
      "        mm = sents_feat * dc  # sentences feature vectors \\times diagonal of coeficients. sentences by features\n",
      "        return mm, sents, sents_feat\n",
      "\n",
      "\n",
      "def score_top_feat(pool, sent_detector, score_model, vcn):\n",
      "    test_sent = []\n",
      "\n",
      "    list_pool = list(pool.remaining)\n",
      "    # indices = rand.permutation(len(pool.remaining))\n",
      "    # remaining = [list_pool[index] for index in indices]\n",
      "    target_sent = []\n",
      "    for i in list_pool:\n",
      "        mm, _, sent_bow = sentence2values(pool.text[i], sent_detector, score_model, vcn)\n",
      "        max_vals = np.argmax(mm.max(axis=1))\n",
      "\n",
      "        if isinstance(test_sent, list):\n",
      "            test_sent = sent_bow[max_vals]\n",
      "        else:\n",
      "            test_sent = vstack([test_sent, sent_bow[max_vals]], format='csr')\n",
      "        target_sent.append(pool.target[i])\n",
      "    return test_sent, target_sent\n",
      "\n",
      "\n",
      "def split_data_sentences(data, sent_detector, vct, limit=0):\n",
      "    sent_train = []\n",
      "    labels = []\n",
      "    tokenizer = vct.build_tokenizer()\n",
      "    size_per_doc = []\n",
      "    print (\"Spliting into sentences... Limit:\", limit)\n",
      "    ## Convert the documents into sentences: train\n",
      "    for t, sentences in zip(data.train.target, sent_detector.batch_tokenize(data.train.data)):\n",
      "\n",
      "        if limit is None:\n",
      "            sents = [s for s in sentences if len(tokenizer(s)) > 1]\n",
      "        elif limit > 0:\n",
      "            sents = [s for s in sentences if len(s.strip()) > limit]\n",
      "            size_per_doc.append(len(sentences))\n",
      "        elif limit == 0:\n",
      "            sents = [s for s in sentences]\n",
      "        sent_train.extend(sents)  # at the sentences separately as individual documents\n",
      "        labels.extend([t] * len(sents))  # Give the label of the document to all its sentences\n",
      "\n",
      "    return labels, sent_train, size_per_doc\n",
      "\n",
      "def distribution_per_sentence(data, sent_detector, vct):\n",
      "    t0 = time.time()\n",
      "\n",
      "    print \"Sentence distribution\"\n",
      "\n",
      "    X, y, sent_per_doc = split_data_sentences(data, sent_detector, vct, limit=2)\n",
      "\n",
      "    sent_per_doc = np.array(sent_per_doc)\n",
      "    sents = Counter(sent_per_doc)\n",
      "\n",
      "    n, bins, patches = plt.hist(sents.keys(), weights=sents.values(),  bins=50)\n",
      "    plt.title(\"Document Number of Sentence Distribution {0} (mean={1:.2f}) N={2}\".format(args.train, sent_per_doc.mean(),\n",
      "                                                                                         len(sent_per_doc)),\n",
      "              fontsize=14, fontfamily='Aria')\n",
      "    plt.xlabel(\"Number of sentences\")\n",
      "    plt.ylabel(\"Frequency\")\n",
      "    plt.show()\n",
      "    for i,b in enumerate(bins):\n",
      "        print \"bin %s: %s\" % (i,b)\n",
      "\n",
      "    print(\"Elapsed time %.3f\" % (time.time() - t0))\n",
      "    return sent_per_doc\n",
      "    \n",
      "    \n",
      "rand = np.random.mtrand.RandomState(args.seed)\n",
      "\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1, 1),\n",
      "                  token_pattern='\\\\b\\\\w+\\\\b')  #, tokenizer=StemTokenizer())\n",
      "\n",
      "categories = [['alt.atheism', 'talk.religion.misc'],\n",
      "              ['comp.graphics', 'comp.windows.x'],\n",
      "              ['comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware'],\n",
      "              ['rec.sport.baseball', 'sci.crypt']]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = get_data(\"imdb\", [categories[0]], vct, True)  # expert: classifier, data contains train and test\n",
      "\n",
      "sent_per_doc = distribution_per_sentence(data, sent_detector, vct)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}