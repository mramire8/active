{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'mramire8'\n",
      "__copyright__ = \"Copyright 2013, ML Lab\"\n",
      "__version__ = \"0.2\"\n",
      "__status__ = \"Development\"\n",
      "\n",
      "import sys\n",
      "import os\n",
      "\n",
      "sys.path.append(os.path.abspath(\".\"))\n",
      "sys.path.append(os.path.abspath(\"../\"))\n",
      "sys.path.append(os.path.abspath(\"../experiment/\"))\n",
      "\n",
      "\n",
      "from experiment.experiment_utils import split_data_sentences, parse_parameters_mat, clean_html, set_cost_model\n",
      "import argparse\n",
      "import numpy as np\n",
      "from sklearn.datasets.base import Bunch\n",
      "from datautil.load_data import load_from_file, split_data\n",
      "from sklearn import linear_model\n",
      "import time\n",
      "\n",
      "from collections import defaultdict\n",
      "from strategy import structured\n",
      "from expert import baseexpert\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "import random\n",
      "import nltk\n",
      "from scipy.sparse import vstack\n",
      "from sklearn import metrics\n",
      "from learner.adaptive_lr import LogisticRegressionAdaptive\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "rand = np.random.mtrand.RandomState(args.seed)\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "\n",
      "\n",
      "def print_features(coef, names):\n",
      "    \"\"\" Print sorted list of non-zero features/weights. \"\"\"\n",
      "    print \"\\n\".join('%s/%.2f' % (names[j], coef[j]) for j in np.argsort(coef)[::-1] if coef[j] != 0)\n",
      "\n",
      "\n",
      "def get_data(clf, train, cats, fixk, min_size, vct, raw):\n",
      "    import copy\n",
      "    min_size = 10\n",
      "\n",
      "    args.fixk = None\n",
      "\n",
      "    data, vct2 = load_from_file(train, cats, fixk, min_size, vct, raw=raw)\n",
      "\n",
      "    print(\"Data %s\" % args.train)\n",
      "    print(\"Data size %s\" % len(data.train.data))\n",
      "\n",
      "    parameters = parse_parameters_mat(args.cost_model)\n",
      "\n",
      "    print \"Cost Parameters %s\" % parameters\n",
      "\n",
      "    cost_model = set_cost_model(args.cost_function, parameters=parameters)\n",
      "    print \"\\nCost Model: %s\" % cost_model.__class__.__name__\n",
      "\n",
      "    ### SENTENCE TRANSFORMATION\n",
      "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "    ## delete <br> to \".\" to recognize as end of sentence\n",
      "    data.train.data = clean_html(data.train.data)\n",
      "    data.test.data = clean_html(data.test.data)\n",
      "\n",
      "    print(\"Train:{}, Test:{}, {}\".format(len(data.train.data), len(data.test.data), data.test.target.shape[0]))\n",
      "    ## Get the features of the sentence dataset\n",
      "\n",
      "    ## create splits of data: pool, test, oracle, sentences\n",
      "    expert_data = Bunch()\n",
      "    train_test_data = Bunch()\n",
      "\n",
      "    expert_data.sentence, train_test_data.pool = split_data(data.train)\n",
      "    expert_data.oracle, train_test_data.test = split_data(data.test)\n",
      "\n",
      "    data.train.data = train_test_data.pool.train.data\n",
      "    data.train.target = train_test_data.pool.train.target\n",
      "\n",
      "    data.test.data = train_test_data.test.train.data\n",
      "    data.test.target = train_test_data.test.train.target\n",
      "\n",
      "    ## convert document to matrix\n",
      "    data.train.bow = vct.fit_transform(data.train.data)\n",
      "    data.test.bow = vct.transform(data.test.data)\n",
      "\n",
      "    #### EXPERT CLASSIFIER: ORACLE\n",
      "    print(\"Training Oracle expert\")\n",
      "\n",
      "    labels, sent_train = split_data_sentences(expert_data.oracle.train, sent_detector, vct, limit=2)\n",
      "    print len(sent_train)\n",
      "    expert_data.oracle.train.data = sent_train\n",
      "    expert_data.oracle.train.target = np.array(labels)\n",
      "    expert_data.oracle.train.bow = vct.transform(expert_data.oracle.train.data)\n",
      "    print expert_data.oracle.train.bow.shape\n",
      "    # exp_clf = linear_model.LogisticRegression(penalty='l1', C=args.expert_penalty)\n",
      "    exp_clf = copy.copy(clf)\n",
      "    exp_clf.fit(expert_data.oracle.train.bow, expert_data.oracle.train.target)\n",
      "\n",
      "    #### EXPERT CLASSIFIER: SENTENCES\n",
      "    print(\"Training sentence expert\")\n",
      "    labels, sent_train = split_data_sentences(expert_data.sentence.train, sent_detector, vct, limit=2)\n",
      "\n",
      "    expert_data.sentence.train.data = sent_train\n",
      "    expert_data.sentence.train.target = np.array(labels)\n",
      "    expert_data.sentence.train.bow = vct.transform(expert_data.sentence.train.data)\n",
      "\n",
      "    sent_clf = None\n",
      "    # if args.cheating:\n",
      "    sent_clf = copy.copy(clf)\n",
      "    # sent_clf = linear_model.LogisticRegression(penalty='l1', C=args.expert_penalty)\n",
      "    sent_clf.fit(expert_data.sentence.train.bow, expert_data.sentence.train.target)\n",
      "\n",
      "    return exp_clf, data, vct, cost_model, sent_clf,  expert_data\n",
      "\n",
      "\n",
      "####################### MAIN ####################\n",
      "def get_sentences_by_method(pool, student, test_sent):\n",
      "    test_sent = []\n",
      "\n",
      "    list_pool = list(pool.remaining)\n",
      "    # indices = rand.permutation(len(pool.remaining))\n",
      "    # remaining = [list_pool[index] for index in indices]\n",
      "    target_sent = []\n",
      "    text_sent = []\n",
      "    all_scores = []\n",
      "    order_sent=[]\n",
      "    for i in list_pool:\n",
      "        scores, sent_bow, sent_txt, order = student.x_utility(pool.data[i], pool.text[i])\n",
      "        if isinstance(test_sent, list):\n",
      "            test_sent = sent_bow\n",
      "        else:\n",
      "            test_sent = vstack([test_sent, sent_bow], format='csr')\n",
      "        text_sent.append(sent_txt)\n",
      "        target_sent.append(pool.target[i])\n",
      "        order_sent.append(order)\n",
      "        all_scores.append(scores)\n",
      "    return test_sent, target_sent, text_sent, all_scores, order_sent\n",
      "\n",
      "\n",
      "def calibrate_scores(n_scores, bounds=(.5,1)):\n",
      "    delta = 1.* (bounds[1] - bounds[0]) / (n_scores -1)\n",
      "    calibrated = (np.ones(n_scores)*bounds[1]) - (np.array(range(n_scores))*delta)\n",
      "    return calibrated\n",
      "\n",
      "\n",
      "def reshape_scores(scores, sent_mat):\n",
      "    sr = []\n",
      "    i = 0\n",
      "    for row in sent_mat:\n",
      "        sc = []\n",
      "        for col in row:\n",
      "            sc.append(scores[i])\n",
      "            i = i+1\n",
      "        sr.append(sc)\n",
      "    return np.array(sr)\n",
      "\n",
      "\n",
      "def get_sentences_by_method_cal(pool, student, test_sent):\n",
      "    test_sent = []\n",
      "\n",
      "    list_pool = list(pool.remaining)\n",
      "    # indices = rand.permutation(len(pool.remaining))\n",
      "    # remaining = [list_pool[index] for index in indices]\n",
      "    target_sent = []\n",
      "    text_sent = []\n",
      "    all_scores = []\n",
      "    docs = []\n",
      "    for i in list_pool:\n",
      "        utilities, sent_bow, sent_txt = student.x_utility_cal(pool.data[i], pool.text[i])  # utulity for every sentences in document\n",
      "        all_scores.extend(utilities) ## every score\n",
      "        docs.append(sent_bow)  ## sentences for each document\n",
      "        text_sent.append(sent_txt)  ## text sentences for each document\n",
      "        target_sent.append(pool.target[i])   # target of every document, ground truth\n",
      "\n",
      "    ## Calibrate scores\n",
      "    n = len(all_scores)\n",
      "    all_scores = np.array(all_scores)\n",
      "    break_point = 2\n",
      "    order = all_scores.argsort()[::-1] ## descending order\n",
      "    ## generate scores equivalent to max prob\n",
      "    a = calibrate_scores(n/break_point, bounds=(.5,1))\n",
      "    a = np.append(a, calibrate_scores(n - n/break_point, bounds=(1,.5)))\n",
      "\n",
      "    ## new scores assigned to original sentence order\n",
      "    new_scores = np.zeros(n)\n",
      "    new_scores[order] = a\n",
      "    cal_scores = reshape_scores(new_scores, docs)\n",
      "\n",
      "    selected_sent = [np.argmax(row) for row in cal_scores] ## get the sentence of the highest score per document\n",
      "    selected = [docs[i][k] for i, k in enumerate(selected_sent)]  ## get the bow\n",
      "    selected_score = [np.max(row) for i, row in enumerate(cal_scores)]  ## get the bow\n",
      "    test_sent = list_to_sparse(selected)\n",
      "\n",
      "    return test_sent, np.array(selected_score), selected_sent\n",
      "\n",
      "\n",
      "def list_to_sparse(selected):\n",
      "    test_sent = []\n",
      "    for s in selected:\n",
      "        if isinstance(test_sent, list):\n",
      "            test_sent = s\n",
      "        else:\n",
      "            test_sent = vstack([test_sent, s], format='csr')\n",
      "    return test_sent\n",
      "\n",
      "\n",
      "def get_sentences_by_method_cal_scale(pool, student, test_sent, class_sensitive=True):\n",
      "    from sklearn import preprocessing \n",
      "    test_sent = []\n",
      "\n",
      "    list_pool = list(pool.remaining)\n",
      "    # indices = rand.permutation(len(pool.remaining))\n",
      "    # remaining = [list_pool[index] for index in indices]\n",
      "    target_sent = []\n",
      "    text_sent = []\n",
      "    all_scores = []\n",
      "    all_p0 = []\n",
      "    docs = []\n",
      "    for i in list_pool:\n",
      "        utilities, sent_bow, sent_txt = student.x_utility_cal(pool.data[i], pool.text[i])  # utulity for every sentences in document\n",
      "        all_scores.extend(utilities[::-1) ## every score\n",
      "        docs.append(sent_bow[::-1])  ## sentences for each document\n",
      "        text_sent.append(sent_txt[::-1])  ## text sentences for each document\n",
      "        target_sent.append(pool.target[i])   # target of every document, ground truth\n",
      "        all_p0.extend([student.sent_model.predict_proba(s)[0][0] for s in sent_bow])\n",
      "    ## Calibrate scores\n",
      "\n",
      "    n = len(all_scores)\n",
      "    if n != len(all_p0):\n",
      "        raise Exception(\"Oops there is something wrong! We don't have the same size\")\n",
      "\n",
      "    all_p0 = np.array(all_p0)\n",
      "\n",
      "    order = all_p0.argsort()[::-1] ## descending order\n",
      "    ## generate scores equivalent to max prob\n",
      "    ordered_p0 = all_p0[order]\n",
      "    # class_sensitive = True\n",
      "    if class_sensitive:\n",
      "        c0_scores = preprocessing.scale(ordered_p0[ordered_p0 > .5])\n",
      "        c1_scores = -1. * preprocessing.scale(ordered_p0[ordered_p0 <= .5])\n",
      "        a = np.concatenate((c0_scores, c1_scores))\n",
      "    else:\n",
      "        a = preprocessing.scale(ordered_p0)\n",
      "\n",
      "    new_scores = np.zeros(n)\n",
      "    new_scores[order] = a\n",
      "    cal_scores = reshape_scores(new_scores, docs)\n",
      "    p0 = reshape_scores(all_p0, docs)\n",
      "\n",
      "    selected_sent = [np.argmax(row) for row in cal_scores] ## get the sentence of the highest score per document\n",
      "    selected = [docs[i][k] for i, k in enumerate(selected_sent)]  ## get the bow\n",
      "    selected_score = [np.max(row) for i, row in enumerate(cal_scores)]  ## get the bow\n",
      "    selected_cl = [p0[i][k] for i, k in enumerate(selected_sent)]\n",
      "    test_sent = list_to_sparse(selected)\n",
      "\n",
      "    return test_sent, np.array(selected_score), selected_cl, selected_sent\n",
      "\n",
      "from scipy.sparse import diags\n",
      "\n",
      "\n",
      "def sentence2values(doc_text, sent_detector, score_model, vcn):\n",
      "        np.set_printoptions(precision=4)\n",
      "        sents = sent_detector.tokenize(doc_text)\n",
      "        sents_feat = vcn.transform(sents)\n",
      "        coef = score_model.coef_[0]\n",
      "        dc = diags(coef, 0)\n",
      "\n",
      "        mm = sents_feat * dc  # sentences feature vectors \\times diagonal of coeficients. sentences by features\n",
      "        return mm, sents, sents_feat\n",
      "\n",
      "\n",
      "def score_top_feat(pool, sent_detector, score_model, vcn):\n",
      "    test_sent = []\n",
      "\n",
      "    list_pool = list(pool.remaining)\n",
      "    # indices = rand.permutation(len(pool.remaining))\n",
      "    # remaining = [list_pool[index] for index in indices]\n",
      "    target_sent = []\n",
      "    for i in list_pool:\n",
      "        mm, _, sent_bow = sentence2values(pool.text[i], sent_detector, score_model, vcn)\n",
      "        max_vals = np.argmax(mm.max(axis=1))\n",
      "\n",
      "        if isinstance(test_sent, list):\n",
      "            test_sent = sent_bow[max_vals]\n",
      "        else:\n",
      "            test_sent = vstack([test_sent, sent_bow[max_vals]], format='csr')\n",
      "        target_sent.append(pool.target[i])\n",
      "    return test_sent, target_sent\n",
      "\n",
      "\n",
      "def score_distribution(calibrated, pool, sent_data, student, sizes, cheating=False, show=False):\n",
      "\n",
      "    print \"Testing size: %s\" % len(pool.target)\n",
      "    print \"Class distribution: %s\" % (1. * pool.target.sum() / len(pool.target))\n",
      "    # # Sentence dataset\n",
      "    train_sent = sent_data.oracle.train\n",
      "    import copy\n",
      "    ## don't care utility of document\n",
      "    student.fn_utility = student.utility_one\n",
      "    ## only testing distribution with max score of the student sentence model\n",
      "    fns = [student.score_max]\n",
      "    # fns = [student.score_rnd]\n",
      "    results = defaultdict(lambda: [])\n",
      "    for size in sizes:\n",
      "\n",
      "        # train underlying sentence classifier of the student\n",
      "        if not cheating:\n",
      "            clf_test = copy.copy(student.sent_model)\n",
      "            clf_test.fit(train_sent.bow[:size], train_sent.target[:size])\n",
      "            student.set_sentence_model(clf_test)\n",
      "\n",
      "        clf_test = student.sent_model\n",
      "\n",
      "        for fn in fns:\n",
      "            test_sent = []\n",
      "            student.score = fn\n",
      "\n",
      "            ## for every document pick a sentence\n",
      "            if calibrated == 'zscore':\n",
      "                test_sent, scores, ori_scores, sel_sent = get_sentences_by_method_cal_scale(pool, student, test_sent)\n",
      "                if show:\n",
      "                    plot_histogram(sel_sent, \"Zcores\", show=True)\n",
      "            elif calibrated == 'uniform':\n",
      "                test_sent, scores, sel_sent = get_sentences_by_method_cal(pool, student, test_sent)\n",
      "                if show:\n",
      "                    plot_histogram(sel_sent, \"Uniform\", show=True)\n",
      "            else:\n",
      "                test_sent, _, _, scores, sel_sent = get_sentences_by_method(pool, student, test_sent)\n",
      "                if show:\n",
      "                    plot_histogram(sel_sent, calibrated, show=True)\n",
      "                # pred_prob = clf_test.predict_proba(test_sent)\n",
      "                # scores = pred_prob[:,0]\n",
      "\n",
      "            predict = clf_test.predict(test_sent)\n",
      "            mname = fn.__name__\n",
      "            print \"-\" * 40\n",
      "            test_name = \"caliball-{}-size-{}\".format(mname, size)\n",
      "            print test_name\n",
      "            if show:\n",
      "                plot_histogram([scores[pool.target == 0], scores[pool.target == 1]], test_name, show=False)\n",
      "            score_confusion_matrix(pool.target, predict, [0, 1])\n",
      "            accu = metrics.accuracy_score(pool.target, predict)\n",
      "            print \"Accu %s \\t%s\" % (test_name, accu)\n",
      "            results[size].append(sel_sent)\n",
      "    return results\n",
      "\n",
      "\n",
      "def other_distribution(exp_clf, fns, pool, sent_clf, student, vct):\n",
      "    # # create data for testing method\n",
      "    # select the first sentence always\n",
      "    print args.train\n",
      "    print \"Testing size: %s\" % len(pool.target)\n",
      "    print \"Class distribution: %s\" % (1. * pool.target.sum() / len(pool.target))\n",
      "    student.fn_utility = student.utility_one\n",
      "    # clf_test = clf\n",
      "    # clf_test.fit(pool.data, pool.target)\n",
      "    # student.set_sentence_model(clf_test)\n",
      "    clf_test = sent_clf\n",
      "    offset = 0\n",
      "    for fn in fns:\n",
      "        ## firstk\n",
      "        test_sent = []\n",
      "        student.score = fn\n",
      "        test_sent, target_sent, text_sent = get_sentences_by_method(pool, student, test_sent)\n",
      "        predict = clf_test.predict(test_sent)\n",
      "        pred_prob = clf_test.predict_proba(test_sent)\n",
      "        mname = fn.__name__\n",
      "        plot_histogram(pred_prob[:, 0], mname)\n",
      "        # print \"METHOD: %s\" % fn.__name__\n",
      "\n",
      "        if False:\n",
      "            print_document(text_sent, offset, method_name=mname, top=500, truth=pool.target,\n",
      "                           prediction=predict)  #, org_doc=pool.text)\n",
      "        offset += 500\n",
      "        # accu = metrics.accuracy_score(pool.target, predict)\n",
      "        print mname\n",
      "        score_confusion_matrix(pool.target, predict, [0, 1])\n",
      "        #print \"Accu %s \\t%s\" % (student.score.__name__, accu)\n",
      "    if False:  ## show top feature method\n",
      "        test_sent, target_sent = score_top_feat(pool, sent_detector, exp_clf, vct)\n",
      "        predict = clf_test.predict(test_sent)\n",
      "\n",
      "        accu = metrics.accuracy_score(pool.target, predict)\n",
      "        print \"Accu %s \\t%s\" % (score_top_feat.__name__, accu)\n",
      "\n",
      "\n",
      "def main():\n",
      "    test_methods = False\n",
      "    test_distribution = True\n",
      "    sent_average = False\n",
      "    sizes = range(1000, 20000, 5000)\n",
      "\n",
      "    # vct = CountVectorizer(encoding='ISO-8859-1', min_df=5, max_df=1.0, binary=True, ngram_range=(1, 3),\n",
      "    #                       token_pattern='\\\\b\\\\w+\\\\b')#, tokenizer=StemTokenizer())\n",
      "\n",
      "    vct = TfidfVectorizer(encoding='latin1', min_df=5, max_df=1.0, binary=False, ngram_range=(1, 1),\n",
      "                      token_pattern='\\\\b\\\\w+\\\\b')  #, tokenizer=StemTokenizer())\n",
      "\n",
      "    print(\"Start loading ...\")\n",
      "\n",
      "    ########## NEWS GROUPS ###############\n",
      "    # easy to hard. see \"Less is More\" paper: http://axon.cs.byu.edu/~martinez/classes/678/Presentations/Clawson.pdf\n",
      "    categories = [['alt.atheism', 'talk.religion.misc'],\n",
      "                  ['comp.graphics', 'comp.windows.x'],\n",
      "                  ['comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware'],\n",
      "                  ['rec.sport.baseball', 'sci.crypt']]\n",
      "\n",
      "    min_size = max(10, args.fixk)\n",
      "\n",
      "    if args.fixk < 0:\n",
      "        args.fixk = None\n",
      "\n",
      "    # clf = linear_model.LogisticRegression(penalty='l1', C=args.expert_penalty)\n",
      "    clf = LogisticRegressionAdaptive(penalty='l1', C=1)\n",
      "\n",
      "    exp_clf, data, vct, cost_model, sent_clf, sent_data = get_data(clf, args.train, [categories[0]], args.fixk, min_size, vct, raw=True)  # expert: classifier, data contains train and test\n",
      "    print \"\\nExpert: %s \" % exp_clf\n",
      "\n",
      "    print (\"Sentences scoring\")\n",
      "    t0 = time.time()\n",
      "    ### experiment starts\n",
      "\n",
      "    student = structured.AALStructuredReading(model=clf, accuracy_model=None, budget=args.budget, seed=args.seed, vcn=vct,\n",
      "                                              subpool=250, cost_model=cost_model)\n",
      "    student.set_score_model(exp_clf)  # expert model\n",
      "    student.set_sentence_model(sent_clf)  # expert sentence model\n",
      "    student.limit = 2\n",
      "    print \"Expert: :\", exp_clf\n",
      "    print \"Sentence:\", sent_clf\n",
      "\n",
      "    coef = exp_clf.coef_[0]\n",
      "    feats = vct.get_feature_names()\n",
      "    print \"*\" * 60\n",
      "    # print_features(coef, feats)\n",
      "    print \"*\" * 60\n",
      "\n",
      "    pool = Bunch()\n",
      "    pool.data = data.train.bow.tocsr()   # full words, for training\n",
      "    pool.text = data.train.data\n",
      "    pool.target = data.train.target\n",
      "    pool.predicted = []\n",
      "    pool.remaining = range(pool.data.shape[0])  # indices of the pool\n",
      "\n",
      "    if sent_average:\n",
      "        print sentences_average(pool, vct)\n",
      "\n",
      "    fns = [student.score_fk, student.score_max, student.score_rnd, student.score_max_feat, student.score_max_sim]\n",
      "    fns = [student.score_max]\n",
      "\n",
      "    if test_methods:\n",
      "        other_distribution(exp_clf, fns, pool, sent_clf, student, vct)  ## get prob. distribution without calibration\n",
      "\n",
      "    calibrated = 'random'\n",
      "    results = []\n",
      "    if test_distribution:\n",
      "        from collections import Counter\n",
      "        ## create data for testing method\n",
      "        # select the first sentence always\n",
      "        # for i in range(5):\n",
      "        for cal in ['uniform', 'zscore']:\n",
      "            results.append(score_distribution(cal, pool, sent_data, student, [1], cheating=True))\n",
      "        avg = Counter()\n",
      "        for r in results:\n",
      "            c = Counter(r[1][0])\n",
      "            for k,v in c.iteritems():\n",
      "                avg[k] += v/10.\n",
      "\n",
      "        plt.hist(avg.keys(), weights=avg.values(), bins=100, align='mid', alpha=.65)\n",
      "        plt.title(\"Random Distributions t=10\", fontsize=12)\n",
      "        plt.xlabel(\"Sentence Location\")\n",
      "        plt.ylabel(\"Frequency\")\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "\n",
      "\n",
      "    print \"Elapsed time %.3f\" % (time.time() - t0)\n",
      "\n",
      "\n",
      "def score_confusion_matrix(true_labels, predicted, labels):\n",
      "    cm = metrics.confusion_matrix(true_labels, predicted, labels=labels)\n",
      "    print \"Predicted -->\"\n",
      "    print \"\\t\" + \"\\t\".join(str(l) for l in np.unique(true_labels))\n",
      "    for l in np.unique(true_labels):\n",
      "        print \"{}\\t{}\".format(l,\"\\t\".join([\"{}\".format(r) for r in cm[l]]))\n",
      "    print \"\\n{}\\t{}\".format(cm[0][0]+cm[1][0],cm[0][1]+cm[1][1],)\n",
      "\n",
      "\n",
      "def plot_histogram(values, title, show=False):\n",
      "\n",
      "    n, bins, patches = plt.hist(values, stacked=True, bins=100, align='mid',label=['y=0', 'y=1'], alpha=.65)\n",
      "\n",
      "    # plt.title(title + ' Distribution $P_{L}(y=0|x)$ $y=0$ (mean=%.2f, N=%d)' % (np.mean(values), len(values)), fontsize=12)\n",
      "    plt.xlabel(\"$P_{L}(\\hat{y}=0|x)$\")\n",
      "    plt.ylabel(\"Frequency\")\n",
      "    plt.legend()\n",
      "    plt.savefig(title+\".png\", bbox_inches=\"tight\", dpi=200, transparent=True)\n",
      "    if show:\n",
      "        plt.show()\n",
      "    else:\n",
      "        plt.clf()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}